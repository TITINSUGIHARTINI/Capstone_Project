{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Machine** **Learning** **MatchSense**"
      ],
      "metadata": {
        "id": "d62pZMegsOHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "F015EJzB3D2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import requirements\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "jVY2jnl0sNIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load Dataset\n",
        "import kagglehub\n",
        "datagede_path = kagglehub.dataset_download('gloriara2/datagede')\n",
        "similaritytext_path = kagglehub.dataset_download('gloriara2/similaritytext')\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = \"/kaggle/working/\"\n",
        "TRAIN_PATH = \"/kaggle/input/datagede/train_balanced_150k.csv\"\n",
        "VAL_PATH = \"/kaggle/input/datagede/val_balanced_10k.csv\"\n",
        "TEST_PATH = \"/kaggle/input/similaritytext/test_cut.csv\"\n",
        "GLOVE_PATH = \"/kaggle/input/glove6b300dtxt/glove.6B.300d.txt\"\n",
        "\n",
        "# Updated model parameters\n",
        "max_length = 40\n",
        "embedding_dim = 300\n",
        "dropout_rate = 0.3\n",
        "lstm_units = 128\n",
        "batch_size = 128\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "81_yDzcsvZZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom TensorFlow Layers"
      ],
      "metadata": {
        "id": "wzl4CVMA9ZC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layers for TensorFlow operations\n",
        "class AbsDiffLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Custom layer to calculate absolute difference between two tensors.\"\"\"\n",
        "    def call(self, inputs):\n",
        "        x1, x2 = inputs\n",
        "        return tf.abs(x1 - x2)\n",
        "\n",
        "class ElementWiseMulLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Custom layer to perform element-wise multiplication of two tensors.\"\"\"\n",
        "    def call(self, inputs):\n",
        "        x1, x2 = inputs\n",
        "        return tf.multiply(x1, x2)\n",
        ""
      ],
      "metadata": {
        "id": "ieSwdmXpBdGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load and Preprocess Data"
      ],
      "metadata": {
        "id": "E5bBSi_DBfbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\"Enhanced data loading and preprocessing\"\"\"\n",
        "    print(\"Loading datasets...\")\n",
        "    df_train = pd.read_csv(TRAIN_PATH)\n",
        "    df_val = pd.read_csv(VAL_PATH)\n",
        "    df_test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "    # Text cleaning\n",
        "    def clean_text(text):\n",
        "        if isinstance(text, str):\n",
        "            text = text.lower().strip()\n",
        "            text = ' '.join(text.split())\n",
        "            return text\n",
        "        return ''\n",
        "\n",
        "    print(\"Cleaning text...\")\n",
        "    for df in [df_train, df_val, df_test]:\n",
        "        df['sentence1'] = df['sentence1'].apply(clean_text)\n",
        "        df['sentence2'] = df['sentence2'].apply(clean_text)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"Tokenizing text...\")\n",
        "    tokenizer = Tokenizer(\n",
        "        num_words=40000,\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "        lower=True,\n",
        "        oov_token='<UNK>'\n",
        "    )\n",
        "\n",
        "    all_sentences = np.concatenate([\n",
        "        df_train['sentence1'].values, df_train['sentence2'].values,\n",
        "        df_val['sentence1'].values, df_val['sentence2'].values,\n",
        "        df_test['sentence1'].values, df_test['sentence2'].values\n",
        "    ])\n",
        "\n",
        "    tokenizer.fit_on_texts(all_sentences)\n",
        "\n",
        "    def prepare_data(df):\n",
        "        seq1 = tokenizer.texts_to_sequences(df['sentence1'].values)\n",
        "        seq2 = tokenizer.texts_to_sequences(df['sentence2'].values)\n",
        "        pad1 = pad_sequences(seq1, maxlen=max_length, padding='post', truncating='post')\n",
        "        pad2 = pad_sequences(seq2, maxlen=max_length, padding='post', truncating='post')\n",
        "        labels = df['label'].values.astype('float32')\n",
        "        return pad1, pad2, labels\n",
        "\n",
        "    print(\"Preparing data...\")\n",
        "    train_data = prepare_data(df_train)\n",
        "    val_data = prepare_data(df_val)\n",
        "    test_data = prepare_data(df_test)\n",
        "\n",
        "    return train_data, val_data, test_data, tokenizer\n",
        ""
      ],
      "metadata": {
        "id": "wGfLPWQdBmIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load GloVe Embeddings"
      ],
      "metadata": {
        "id": "z0_HhLnqB3X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings():\n",
        "    \"\"\"Load GloVe embeddings\"\"\"\n",
        "    embeddings_index = {}\n",
        "    print(f\"Loading GloVe embeddings from {GLOVE_PATH}...\")\n",
        "    with open(GLOVE_PATH, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "    return embeddings_index\n",
        ""
      ],
      "metadata": {
        "id": "cq10l33gCBxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Embedding Matrix"
      ],
      "metadata": {
        "id": "6IyZYFHNCDe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(word_index, embeddings_index):\n",
        "    \"\"\"Create embedding matrix\"\"\"\n",
        "    print(\"Creating embedding matrix...\")\n",
        "    vocab_size = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    print(\"Embedding matrix created.\")\n",
        "    return embedding_matrix\n",
        ""
      ],
      "metadata": {
        "id": "8ncdgfauCSsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Model Architecture"
      ],
      "metadata": {
        "id": "_60DZNiYCdip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size, embedding_matrix):\n",
        "    \"\"\"Optimized model architecture\"\"\"\n",
        "    # Input layers\n",
        "    input_1 = Input(shape=(max_length,))\n",
        "    input_2 = Input(shape=(max_length,))\n",
        "\n",
        "    # GloVe embedding layer\n",
        "    embedding = Embedding(\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True,\n",
        "        name='embedding'\n",
        "    )\n",
        "\n",
        "    def encoder_block(x):\n",
        "        # BiLSTM with stronger regularization\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            lstm_units,\n",
        "            return_sequences=True,\n",
        "            dropout=dropout_rate,\n",
        "            recurrent_dropout=0.2,\n",
        "            kernel_regularizer=l2(1e-5),\n",
        "        ))(x)\n",
        "\n",
        "        # Layer normalization\n",
        "        lstm = LayerNormalization(epsilon=1e-6)(lstm)\n",
        "\n",
        "        # Self-attention\n",
        "        attention = Dense(1, activation='relu')(lstm)\n",
        "        attention = Flatten()(attention)\n",
        "        attention = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(lstm_units * 2)(attention)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        # Residual connection\n",
        "        attended = multiply([lstm, attention])\n",
        "        output = Add()([lstm, attended])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def encode_sequence(x):\n",
        "        x = embedding(x)\n",
        "        x = encoder_block(x)\n",
        "\n",
        "        # Multiple pooling with normalization\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "\n",
        "        # Normalize pooled features\n",
        "        avg_pool = LayerNormalization(epsilon=1e-6)(avg_pool)\n",
        "        max_pool = LayerNormalization(epsilon=1e-6)(max_pool)\n",
        "\n",
        "        return concatenate([avg_pool, max_pool])\n",
        "\n",
        "    # Encode both input sequences\n",
        "    encoded_1 = encode_sequence(input_1)\n",
        "    encoded_2 = encode_sequence(input_2)\n",
        "\n",
        "    # custom layers\n",
        "    abs_diff = AbsDiffLayer()([encoded_1, encoded_2])\n",
        "    mul = ElementWiseMulLayer()([encoded_1, encoded_2])\n",
        "\n",
        "    # Combine features\n",
        "    merged = concatenate([encoded_1, encoded_2, abs_diff, mul])\n",
        "\n",
        "    # Dense layers with stronger regularization and normalization\n",
        "    x = merged\n",
        "    for units in [512, 256, 128]:\n",
        "        x = Dense(units, kernel_regularizer=l2(1e-4))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "    # Output with increased regularization\n",
        "    output = Dense(1, activation='sigmoid', kernel_regularizer=l2(1e-4))(x)\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    # Optimized learning rate schedule\n",
        "    initial_learning_rate = 1e-3\n",
        "    decay_steps = 5000\n",
        "    min_lr = 1e-6\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps,\n",
        "        end_learning_rate=min_lr,\n",
        "        power=1.0\n",
        "    )\n",
        "\n",
        "    # Optimizer with increased weight decay\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=lr_schedule,\n",
        "        weight_decay=1e-4,\n",
        "        clipnorm=1.0\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3bEnjO79CihS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function"
      ],
      "metadata": {
        "id": "mBT96B3jC5vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    embeddings_index = load_glove_embeddings()\n",
        "    train_data, val_data, test_data, tokenizer = load_and_preprocess_data()\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    embedding_matrix = create_embedding_matrix(tokenizer.word_index, embeddings_index)\n",
        "    model = create_model(vocab_size, embedding_matrix)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(BASE_PATH, 'best_model.keras'),\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_data[2],\n",
        "        validation_data=([val_data[0], val_data[1]], val_data[2]),\n",
        "        epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model.save(os.path.join(BASE_PATH, 'final_model.h5'))\n",
        "    with open(os.path.join(BASE_PATH, 'tokenizer.json'), 'w') as f:\n",
        "        f.write(json.dumps(tokenizer.to_json()))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(BASE_PATH, 'training_history.png'))\n",
        "\n",
        "    return model, history, tokenizer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, history, tokenizer = main()\n"
      ],
      "metadata": {
        "id": "aOyTf6Y5Ctli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "1RSRANiC9oRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_data(model, test_path):\n",
        "    \"\"\"Function to predict and display test data results.\"\"\"\n",
        "    print(\"Predicting test data...\")\n",
        "\n",
        "    # Load test data\n",
        "    print(\"Loading test data...\")\n",
        "    df_test = pd.read_csv(test_path)\n",
        "\n",
        "    def clean_text(text):\n",
        "        if isinstance(text, str):\n",
        "            text = text.lower().strip()\n",
        "            text = ' '.join(text.split())\n",
        "            return text\n",
        "        return ''\n",
        "\n",
        "    # Clean text data\n",
        "    print(\"Cleaning test data...\")\n",
        "    df_test['sentence1'] = df_test['sentence1'].apply(clean_text)\n",
        "    df_test['sentence2'] = df_test['sentence2'].apply(clean_text)\n",
        "\n",
        "    # Tokenize text data\n",
        "    print(\"Encoding test data...\")\n",
        "    seq1 = tokenizer.texts_to_sequences(df_test['sentence1'].values)\n",
        "    seq2 = tokenizer.texts_to_sequences(df_test['sentence2'].values)\n",
        "    pad1 = pad_sequences(seq1, maxlen=max_length, padding='post', truncating='post')\n",
        "    pad2 = pad_sequences(seq2, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Predict similarity scores\n",
        "    print(\"Predicting...\")\n",
        "    predictions = model.predict([pad1, pad2], batch_size=batch_size, verbose=1)\n",
        "    df_test['similarity_score'] = predictions\n",
        "    df_test['predicted_label'] = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Save predictions to file\n",
        "    output_path = os.path.join(BASE_PATH, 'test_predictions.csv')\n",
        "    df_test.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions saved to {output_path}!\")\n",
        "\n",
        "    return df_test\n"
      ],
      "metadata": {
        "id": "02F9_2IipSp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Panggil fungsi prediksi\n",
        "test_results = predict_test_data(model, TEST_PATH)\n",
        "\n",
        "# Tampilkan beberapa baris hasil prediksi\n",
        "print(\"Test data predictions completed!\")\n",
        "print(test_results.head())\n"
      ],
      "metadata": {
        "id": "XwjMAwkbpWPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}